{"Procedure":{"Name":"NNET","ProductGroup":"CAS","ProcedureHelp":{"#cdata":"Syntax: PROC NNET < options > ;\n    INPUT variables < / LEVEL=INT | NOM> ;\n    HIDDEN number < /options > ;\n    TARGET variables < /options > ;\n    TRAIN OUTMODEL=CAS-libref.data-table < options > ;\n    ARCHITECTURE architecture-options ;\n    WEIGHT variable ;\n    PARTITION < partition-options > ;\n    OPTIMIZATION < options > ;\n    AUTOTUNE < options > ;\n    CROSSVALIDATION <KFOLD=number > ;\n    SCORE OUT=CAS-libref.data-table < option > ;\n    CODE < options > ; \n  \nThe NNET procedure trains a multilayer perceptron neural network in SAS Viya. For more information\nabout multilayer perceptron neural networks, see Bishop (1995). PROC NNET can also use a previously\ntrained network to score a data table (referred to as stand-alone scoring), or it can generate SAS DATA step\nstatements that can be used to score a data table."},"ProcedureOptions":{"ProcedureOption":[{"ProcedureOptionName":"DATA=","ProcedureOptionHelp":{"#cdata":"Syntax: DATA=CAS-libref.data-table\n      \nNames the input data table for PROC NNET to use. The default is the most recently created data table.\nCAS-libref.data-table is a two-level name, where\n\n  CAS-libref refers to a collection of information that is defined in the LIBNAME statement and\n    includes the caslib, which includes a path to the data, and a session identifier, which\n    defaults to the active session but which can be explicitly defined in the LIBNAME\n    statement.\n    \n  data-table specifies the name of the input data table."},"ProcedureOptionType":"DV"},{"ProcedureOptionName":"INMODEL=","ProcedureOptionHelp":{"#cdata":"Syntax: INMODEL=< CAS-libref. >data-table\n      \nSpecifies a model for stand-alone scoring or coding."},"ProcedureOptionType":"V"},{"ProcedureOptionName":"MISSING=","ProcedureOptionHelp":{"#cdata":"Syntax: MISSING= MIN|MAX|MEAN  \n      \nSpecifies the statistic with which to impute missing values for interval inputs. If you specify this option,\nmissing values for nominal inputs and targets are both treated as a valid level.\nBy default, observations that have missing values for input variables are excluded from the analysis."},"ProcedureOptionType":"V","ProcedureOptionValues":{"@Value1":"MIN","@Value2":"MAX","@Value3":"MEAN"},"ProcedureOptionToolTips":{"@ToolTip1":"Specifies the MIN statistic.","@ToolTip2":"Specifies the MAX statistic.","@ToolTip3":"Specifies the MEAN statistic."}},{"ProcedureOptionName":"NTHREADS=","ProcedureOptionHelp":{"#cdata":"Syntax: NTHREADS=number-of-threads\n      \nSpecifies the number of threads to use for the computation. The default value is the number of CPUs\navailable. The value of number-of-threads can be from 1 to 64, inclusive."},"ProcedureOptionType":"V"},{"ProcedureOptionName":"STANDARDIZE=","ProcedureOptionHelp":{"#cdata":"Syntax: STANDARDIZE= NONE | STD | MIDRANGE  \n      \nSpecifies the method to use for standardizing interval inputs.\n\nBy default, STANDARDIZE = STD."},"ProcedureOptionType":"V","ProcedureOptionValues":{"@Value1":"NONE","@Value2":"STD","@Value3":"MIDRANGE"},"ProcedureOptionToolTips":{"@ToolTip1":"Specifies the method in which the variables are not altered.","@ToolTip2":"Specifies the method in which the variables are scaled such that their mean is 0 and the standard deviation is 1.","@ToolTip3":"Specifies the method in which the variables are scaled such that their midrange is 0 and the half-range is 1. That is, the variables have a minimum of \u20131 and a maximum of 1."}}]},"ProcedureStatements":{"ProcedureStatement":[{"StatementName":"ARCHITECTURE","StatementHelp":{"#cdata":"Syntax: ARCHITECTURE architecture-option ;\n      \nThe ARCHITECTURE statement specifies the architecture of the neural network to be trained."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"GLM","StatementOptionHelp":{"#cdata":"Specifies a neural network architecture that has no hidden layers (this is equivalent to a generalized\nlinear model). If you specify this architecture-option, the HIDDEN statement is not allowed."},"StatementOptionType":"S"},{"StatementOptionName":"MLP","StatementOptionHelp":{"#cdata":"Specifies a multilayer perceptron architecture that has one or more hidden layers. This is the default\narchitecture."},"StatementOptionType":"S"},{"StatementOptionName":"MLP DIRECT","StatementOptionHelp":{"#cdata":"Specifies that direct connections between each input and each target neuron be included when the MLP\narchitecture is used."},"StatementOptionType":"S"}]}},{"StatementName":"AUTOTUNE","StatementHelp":{"#cdata":"Syntax: AUTOTUNE < options > ;\n      \nThe AUTOTUNE statement activates the tuning optimization algorithm, which searches for the best hidden\nlayers and regularization parameters based on the problem and specified options. If ALGORITHM=SGD,\nthe algorithm also searches for the best values of the learning rate and annealing rate. When you specify\nAUTOTUNE statement, PROC NNET ignores any HIDDEN statements that are specified. You cannot specify\nthe AUTOTUNE statement with the CROSSVALIDATION statement, the PARTITION statement, or the\nVALIDATION= option in the TRAIN statement."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"FRACTION=","StatementOptionHelp":{"#cdata":"Syntax: FRACTION=number\n          \nSpecifies the fraction of all data to be used for validation, where number must be between 0.01 and\n0.99, inclusive. If you specify this option, the tuner uses a single partition validation for finding the\nobjective value (validation error estimate). This option might not be advisable for small or unbalanced\ndata tables where the random assignment of the validation subset might not provide a good estimate of\nerror. For large, balanced data tables, a single validation partition is usually sufficient for estimating\nerror; a single partition is more efficient than cross validation in terms of the total execution time.\nBy default, FRACTION=0.3. You cannot specify this option in combination with the KFOLD= option."},"StatementOptionType":"V"},{"StatementOptionName":"KFOLD=","StatementOptionHelp":{"#cdata":"Syntax: KFOLD=number\n          \nSpecifies the number of partition folds in the cross validation process, where number must be between\n2 and 20, inclusive. If you specify this option, the tuner uses cross validation to find the objective value.\nIn cross validation, each model evaluation requires number of training executions (on number\u20131 data\nfolds) and number of scoring executions (on 1 hold-out fold). Thus, the evaluation time is increased by\napproximately number. For small to medium data tables or for unbalanced data tables, cross validation\nprovides on average a better representation of error across the entire data table (a better generalization\nerror).\nBy default, KFOLD=5. You cannot specify this option in combination with the FRACTION= option."},"StatementOptionType":"V"},{"StatementOptionName":"MAXEVALS=","StatementOptionHelp":{"#cdata":"Syntax: MAXEVALS=number \n          \nSpecifies the maximum number of configuration evaluations allowed for the tuner, where number\nmust be an integer greater than or equal to 3. When the number of evaluations is reached, the tuner\nterminates the search and returns the results. To produce a single objective function value (validation\nerror estimate), each configuration evaluation requires either a single model training and scoring\nexecution on a validation partition, or a number of training and scoring executions equal to the value of\nthe KFOLD= option for cross validation. The MAXEVALS= option might lead to termination before\nthe value of the MAXITER= option or the MAXTIME= option is reached.\nBy default, MAXEVALS=50."},"StatementOptionType":"V"},{"StatementOptionName":"MAXITER=","StatementOptionHelp":{"#cdata":"Syntax: MAXITER=number\n          \nSpecifies the maximum number of iterations of the optimization tuner, where number must be greater\nthan or equal to 1. Each iteration normally involves a number of objective evaluations up to the value\nof the POPSIZE= option. The MAXITER= option might lead to termination before the value of the\nMAXEVALS= option or the MAXTIME= option is reached.\n\nBy default, MAXITER=5."},"StatementOptionType":"V"},{"StatementOptionName":"MAXTIME=","StatementOptionHelp":{"#cdata":"Syntax: MAXTIME=number\n          \nSpecifies the maximum time (in seconds) allowed for the tuner, where number must be greater than or\nequal to 1. When this value is reached, the tuner terminates the search and returns results. The actual\nrun time for optimization might be longer because it includes the remaining time needed to finish\nthe current evaluation. For long-running model training (large data tables), the actual run time might\nsignificantly exceed number. The MAXTIME= option might lead to termination before the value of\nthe MAXEVALS= option or the MAXITER= option is reached.\nBy default, MAXTIME=36000."},"StatementOptionType":"V"},{"StatementOptionName":"POPSIZE=","StatementOptionHelp":{"#cdata":"Syntax: POPSIZE=number\n          \nSpecifies the maximum number of evaluations in one iteration (population), where number must\nbe greater than or equal to 1. In some cases, the tuner algorithm might generate a number of new\nconfigurations smaller than number.\nBy default, POPSIZE=10."},"StatementOptionType":"V"}]}},{"StatementName":"CODE","StatementHelp":{"#cdata":"Syntax: CODE < options > ;\n      \nThe CODE statement returns the SAS score code that can be used to score data similar to the input data."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"FILE=","StatementOptionHelp":{"#cdata":"Syntax: FILE=filename\n          \nSpecifies the name of the file where PROC NNET is to write the SAS score code."},"StatementOptionType":"V"},{"StatementOptionName":"NOCOMPPGM","StatementOptionHelp":{"#cdata":"Omits the logic of the option FRACTION= option in the PARTITION statement from the score code.\nIf you do not specify this option, the logic of the option FRACTION= option in the PARTITION\nstatement is included in the score code."},"StatementOptionType":"S"}]}},{"StatementName":"CROSSVALIDATION","StatementHelp":{"#cdata":"Syntax: CROSSVALIDATION <KFOLD=number > ;\n      \nThe CROSSVALIDATION statement performs a k-fold cross validation process to find the average estimated\nvalidation error. You cannot specify the CROSSVALIDATION statement if you specify either the\nAUTOTUNE statement or the PARTITION statement."},"StatementOptions":{"StatementOption":{"StatementOptionName":"KFOLD=","StatementOptionHelp":{"#cdata":"Syntax: KFOLD=number\n          \nSpecifies the number of partition folds in the cross validation process, where number must be between\n2 and 20, inclusive.\nBy default, KFOLD=5."},"StatementOptionType":"V"}}},{"StatementName":"HIDDEN","StatementHelp":{"#cdata":"Syntax: HIDDEN number < /options > ;\n      \nThe HIDDEN statement specifies the number of neurons or units in a hidden layer. You can specify multiple\nHIDDEN statements; each HIDDEN statement represents a hidden layer.\n\nHIDDEN statements are ignored when you specify the AUTOTUNE statement."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"ACT=","StatementOptionHelp":{"#cdata":"Syntax: ACT=EXP | IDENTITY | LOGISTIC | SIN | TANH\n          \nSpecifies the activation function for the hidden layer."},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"EXP","@Value2":"IDENTITY","@Value3":"LOGISTIC","@Value4":"SIN","@Value5":"TANH"},"StatementOptionToolTips":{"@ToolTip1":"Specifies the exponential function.","@ToolTip2":"Specifies the identity function.","@ToolTip3":"Specifies the logistic function.","@ToolTip4":"Specifies the sine function.","@ToolTip5":"Specifies the hyperbolic tangent function."}},{"StatementOptionName":"COMB=","StatementOptionHelp":{"#cdata":"Syntax: COMB = ADD | LINEAR   \n          \nSpecifies the combination function for the hidden layer.  \n\nBy default, COMB=LINEAR. "},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"ADD","@Value2":"LINEAR"},"StatementOptionToolTips":{"@ToolTip1":"Specifies the additive combination function.","@ToolTip2":"Specifies the linear combination function."}}]}},{"StatementName":"INPUT","StatementHelp":{"#cdata":"Syntax: INPUT variables < / LEVEL=INT | NOM> ;\n      \nThe INPUT statement identifies the variables in the input data table that are input to the neural network. You\ncan specify multiple INPUT statements."},"StatementOptions":{"StatementOption":{"StatementOptionName":"LEVEL=","StatementOptionHelp":{"#cdata":"Syntax: LEVEL= INT | NOM\n          \nSpecifies the variables in the input data table. \n\nBy default, LEVEL=INT."},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"INT","@Value2":"NOM"},"StatementOptionToolTips":{"@ToolTip1":"Specifies that the variables are interval variables, which must be numeric.","@ToolTip2":"Specifies that the variables are nominal variables, also known as classification variables, which can be numeric or character."}}}},{"StatementName":"OPTIMIZATION","StatementHelp":{"#cdata":"Syntax: OPTIMIZATION < options > ;\n      \nThe OPTIMIZATION statement specifies options for the optimization method that is used to train your model."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"ALGORITHM=","StatementOptionHelp":{"#cdata":"Syntax: ALGORITHM=LBFGS | ALGORITHM=SGD < sgd-options >\n          \nSpecifies the optimization algorithm to use during training."},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"LBFGS","@Value2":"SGD"},"StatementOptionToolTips":{"@ToolTip1":"Specifies the limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm.","@ToolTip2":"Specifies the stochastic gradient descent algorithm. When ALGORITHM=SGD, you can specify these additional sgd-options:  ANNEALINGRATE=number   specifies the annealing parameter, \u03b2. Annealing is a way to automatically reduce the   learning rate as SGD progresses, causing smaller steps as SGD approaches a solution.  COMMFREQ=number   specifies the number of minibatches that each computational thread processes before   weights are synchronized across all threads and nodes.  LEARNINGRATE=number   specifies the learning rate parameter, \u03b7, for SGD.  MINIBATCHSIZE=number   specifies the size of the minibatches used in SGD.  MOMENTUM=number   specifies the value for momentum. The number must be greater than or equal to 0 and   less than or equal to 1. By default, MOMENTUM=0.  SEED=number   specifies the seed for random access of observations on each thread for the SGD algorithm.                                     USELOCKING   specifies that computational threads share a common weight vector and update weight   vector without race conditions."},"SubOptionsKeywords":"ANNEALINGRATE=|COMMFREQ=|LEARNINGRATE=|MINIBATCHSIZE=|MOMENTUM=|SEED=|USELOCKING"},{"StatementOptionName":"MAXITER=","StatementOptionHelp":{"#cdata":"Syntax: MAXITER=number\n          \nSpecifies the maximum number of iterations or epochs before the algorithm terminates.\n\nBy default, MAXITER=250."},"StatementOptionType":"V"},{"StatementOptionName":"MAXTIME=","StatementOptionHelp":{"#cdata":"Syntax: MAXTIME=number\n          \nSpecifies the iteration budget for training. For LBFGS, the algorithm stops after MAXITER= iterations\nif convergence has not been achieved. For SGD, number specifies the desired number of training\nepochs. When MAXTIME=0, no maximum time is set.\n\nBy default, MAXTIME=0."},"StatementOptionType":"V"},{"StatementOptionName":"REGL1=","StatementOptionHelp":{"#cdata":"Syntax: REGL1=number\n          \nSpecifies the L1 regularization parameter 1 for the model loss function. The number must be\nnonnegative. Note that this value is autotuned when you specify the AUTOTUNE statement.\n\nBy default, REGL1=0."},"StatementOptionType":"V"},{"StatementOptionName":"REGL2=","StatementOptionHelp":{"#cdata":"Syntax: REGL2=number\n          \nSpecifies the L2 regularization parameter 2. The number must be nonnegative. Note that this value is\nautotuned when you specify the AUTOTUNE statement.\n\nBy default, REGL2=0."},"StatementOptionType":"V"}]}},{"StatementName":"PARTITION","StatementHelp":{"#cdata":"Syntax: PARTITION partition-option ;\n      \nThe PARTITION statement specifies how observations in the input data set are logically partitioned into\ndisjoint subsets for model training, validation, and testing. Either you can designate a variable in the input\ndata table and a set of formatted values of that variable to determine the role of each observation, or you can\nspecify proportions to use for random assignment of observations for each role. Alternatively, you can use a\nseparate validation data table in the TRAIN statement to do validation."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"FRACTION","StatementOptionHelp":{"#cdata":"Syntax: FRACTION(VALIDATE=fraction TEST=fraction < SEED=random-seed >)\n          \nRandomly assigns the specified proportions of the observations in the input data table to training\nand validation roles. You specify the proportions for testing and validation by using the TEST=\nand VALIDATE= suboptions. The VALIDATE= suboption is required, and the TEST= suboption is\noptional. If you specify both the TEST= and VALIDATE= suboptions, then the sum of the specified\nfractions must be less than 1 and the remaining fraction of the observations are assigned to the training\nrole. Otherwise, the PARTITION statement is ignored. The range of the VALIDATE= and TEST=\nsuboptions is from 1E\u20135 to 1 \u2013 (1E\u20135), inclusive."},"StatementOptionType":"V","SubOptionsKeywords":"VALIDATE=|TEST=|SEED="},{"StatementOptionName":"ROLEVAR=","StatementOptionHelp":{"#cdata":"Syntax: ROLEVAR=variable(TRAIN=value VALIDATE=value < TEST=value >)\n          \nNames the variable in the input data table whose values are used to assign roles to each observation.\nThe formatted values of this variable, which are used to assign observations roles, are specified in the\nTEST=, TRAIN=, and VALIDATION= suboptions. The TRAIN= and VALIDATE= suboptions are\nrequired; the TEST=suboption is optional."},"StatementOptionType":"V","SubOptionsKeywords":"TRAIN=|VALIDATE=|TEST="}]}},{"StatementName":"SCORE","StatementHelp":{"#cdata":"Syntax: SCORE OUT=CAS-libref.data-table < option > ;\n      \nThe SCORE statement creates a new data table that is the result of prediction from using the input data and\nthe model."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"OUT=","StatementOptionHelp":{"#cdata":"Syntax: OUT=CAS-libref.data-table\n          \nNames the output data table for PROC NNET to use. CAS-libref.data-table is a two-level name, where\n\nCAS-libref refers to a collection of information that is defined in the LIBNAME statement and\n  includes the caslib, which includes a path to where the data table is to be stored, and\n  a session identifier, which defaults to the active session but which can be explicitly\n  defined in the LIBNAME statement.\n  \n  data-table specifies the name of the output data table."},"StatementOptionType":"V"},{"StatementOptionName":"COPYVAR=|COPYVARS=","StatementOptionHelp":{"#cdata":"Syntax: COPYVAR=variable | COPYVARS=(variables)\n          \nLists one or more variables from the input data table to be transferred to the output data table."},"StatementOptionType":"V"}]}},{"StatementName":"TARGET","StatementHelp":{"#cdata":"Syntax: TARGET variable < /options > ; \n      \nThe TARGET statement specifies the target variable for the neural network. This statement is required."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"LEVEL=","StatementOptionHelp":{"#cdata":"Syntax: LEVEL= INT | NOM\n          \nSpecifies the variable type."},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"INT","@Value2":"NOM"},"StatementOptionToolTips":{"@ToolTip1":"Specifies that the variable is interval, which must be numeric","@ToolTip2":"Specifies that the variable is nominal, also known as a classification variable, which can be numeric or character."}},{"StatementOptionName":"ACT=","StatementOptionHelp":{"#cdata":"Syntax: ACT=EXP | IDENTITY | SIN | SOFTMAX | TANH  \n          \nSpecifies the activation function for the target. \n\nFor the GLIM architecture, you can only specify ACT=IDENTITY for an interval target and\nACT=SOFTMAX for a nominal target, which are the same by default. For the MLP or MLP DIRECT\narchitecture, the SOFTMAX method is used only with a nominal target, whereas the other\nmethods are used only with an interval targe"},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"EXP","@Value2":"IDENTITY","@Value3":"SIN","@Value4":"SOFTMAX","@Value5":"TANH"},"StatementOptionToolTips":{"@ToolTip1":"Specifies the exponential function. You can use ACT=EXP only with ERROR= GAMMA or ERROR=POISSON.","@ToolTip2":"Specifies the identity function..","@ToolTip3":"Specifies the sine function.","@ToolTip4":"Specifies the softmax function.","@ToolTip5":"Specifies the hyperbolic tangent function."}},{"StatementOptionName":"ERROR=","StatementOptionHelp":{"#cdata":"Syntax: ERROR = ENTROPY | GAMMA | NORMAL | POISSON  \n          \nSpecifies the error function. The entropy error function is used only when LEVEL=NOM."},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"ENTROPY","@Value2":"GAMMA","@Value3":"NORMAL","@Value4":"POISSON"},"StatementOptionToolTips":{"@ToolTip1":"Specifies tthe cross-entropy function.","@ToolTip2":"Specifies the gamma error function. This function is usually used when you want to predict the time  between events. Only ACT=EXP is valid when ERROR=GAMMA.","@ToolTip3":"Specifies the normal error function, which is the sum of the squared differences between the network  output and the target value.","@ToolTip4":"Specifies the Poisson error function. This function is usually used when you want to predict the number of events per unit time. Only ACT=EXP is valid when ERROR=POISSON. "}},{"StatementOptionName":"COMB=","StatementOptionHelp":{"#cdata":"Syntax: COMB = ADD | LINEAR   \n          \nSpecifies the combination function for the target layer.\n\nBy default, COMB=LINEAR. "},"StatementOptionType":"V","StatementOptionValues":{"@Value1":"ADD","@Value2":"LINEAR"},"StatementOptionToolTips":{"@ToolTip1":"Specifies the additive combination function.","@ToolTip2":"Specifies the linear combination function."}}]}},{"StatementName":"TRAIN","StatementHelp":{"#cdata":"Syntax: TRAIN OUTMODEL=CAS-libref.data-table < options > ;\n      \nThe TRAIN statement causes the NNET procedure to use the training data that are specified in the PROC\nNNET statement to train a neural network model whose structure is specified in the ARCHITECTURE,\nINPUT, TARGET, and HIDDEN statements. The goal of training is to determine a set of network weights\nthat best predicts the targets in the training data while still doing a good job of predicting targets \nof unseen data (that is, generalizing well and not overfitting)."},"StatementOptions":{"StatementOption":[{"StatementOptionName":"OUTMODEL=","StatementOptionHelp":{"#cdata":"Syntax: OUTMODEL=CAS-libref.data-table\n          \nSpecifies the final model from training. These parameters include the network architecture, input and\ntarget variable names and types, and trained weights.\nYou can use the model data table later to score a different input data table as long as the variable names\nand types of the variables in the new input data table match those in the training data table."},"StatementOptionType":"V"},{"StatementOptionName":"NUMTRIES=","StatementOptionHelp":{"#cdata":"Syntax: NUMTRIES=number\nspecifies the number of times the network is to be trained using a different starting point. Specifying\nthis option helps ensure that the optimizer finds the table of weights that truly minimizes the objective\nfunction and does not return a local minimum. The value of number must be an integer between 1 and\n20,000, inclusive. By default, NUMTRIES=1.\n\nNOTE: When NUMTRIES > 1, the ODS tables \u201cOptIterHistory\u201d and \u201cConvergenceStatus\u201d are\nsuppressed."},"StatementOptionType":"V"},{"StatementOptionName":"VALIDATION=","StatementOptionHelp":{"#cdata":"Syntax: VALIDATION=CAS-libref.data-table\n          \nSpecifies a separate data table for validation during training. If you specify both the VALIDATION=\nand PARTITION statements, the PARTITION statement is ignored. If neither of these statements is\nspecified, training is done without the validation data table. The VALIDATION= data table must have\nthe same variables that you specify in the DATA= option.\nNOTE: The DATA options keep drop tempnames and tempexpress are not supported."},"StatementOptionType":"V"},{"StatementOptionName":"WSEED=|SEED=","StatementOptionHelp":{"#cdata":"Syntax:WSEED=random-seed | SEED=random-seed\n          \nSpecifies the seed for generating initial random weights. If you do not specify a seed or you specify a\nvalue less than or equal to 0, the seed is generated from the computer clock. This option enables you to\nreproduce the same sample output."},"StatementOptionType":"V"}]}},{"StatementName":"WEIGHT","StatementHelp":{"#cdata":"Syntax: WEIGHT variable;\n      \nIf you specify a WEIGHT statement, variable identifies a numeric variable in the input data table that contains\nthe weight to be placed on the prediction error (the difference between the output of the network and the\ntarget value specified in the input data table) for each observation during training."}}]}}}